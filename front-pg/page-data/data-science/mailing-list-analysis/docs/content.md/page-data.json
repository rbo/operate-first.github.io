{"componentChunkName":"component---src-templates-markdown-js","path":"/data-science/mailing-list-analysis/docs/content.md","result":{"data":{"site":{"siteMetadata":{"title":"Operate First"}},"markdownRemark":{"id":"6d9e36c9-011e-5b54-be3b-f3d28ea36cd1","html":"<h1 id=\"content\" style=\"position:relative;\"><a href=\"#content\" aria-label=\"content permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Content</h1>\n<p>The project repository for mailing list analysis toolkit contains example code for how to develop a custom end-to-end email analytics service using the Open Data Hub on OpenShift.</p>\n<p>Here’s a <a href=\"https://www.youtube.com/watch?v=arvpVoTXgZg\">video</a> which goes over the project and demonstrates the automated dashboard.</p>\n<h2 id=\"current-lists-datasets\" style=\"position:relative;\"><a href=\"#current-lists-datasets\" aria-label=\"current lists datasets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Current Lists/ Datasets</h2>\n<ul class=\"pf-c-list\">\n<li><a href=\"https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/\">Fedora Devel</a></li>\n<li>? (please <a href=\"https://github.com/aicoe-aiops/mailing-list-analysis-toolkit/issues/new?assignees=&#x26;labels=enhancement&#x26;template=feature_request.md\">open an issue</a> if you’d like another mailing list included)</li>\n</ul>\n<h2 id=\"application-overview\" style=\"position:relative;\"><a href=\"#application-overview\" aria-label=\"application overview permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Application Overview</h2>\n<p>At a high level, this application can be seen as an <a href=\"https://argoproj.github.io/argo/\">Argo Workflow</a> which orchestrates a set of Jupyter notebooks to push transformed data to Ceph. Each notebook is responsible for a single task and is used either for collecting raw data from the <a href=\"https://lists.fedoraproject.org/archives/\">Fedora HyperKitty mailing list archive</a> (our live data set), preprocessing that data, or performing some specific analytics task. In almost all cases, the notebooks both push their outputs to Ceph remote storage (for use in a future run) as well as maintain a local copy within a shared volume among the application’s pods for use by other notebook processes. Finally we use external tables in Apache Hive, with Hue, to connect the Ceph data to a SQL database for interactive visualization with Superset.</p>\n<p><img src=\"docs/assets/images/app-overview.png\"></p>\n<p>Here is a <a href=\"../manifests/README.md\">guide</a> which outlines the steps needed to automate your Jupyter notebooks using Argo. By following the steps in the document, your application can be fully set and ready to be deployed via Argo CD.</p>\n<h2 id=\"notebooks\" style=\"position:relative;\"><a href=\"#notebooks\" aria-label=\"notebooks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Notebooks</h2>\n<p>Currently notebooks are divided into two sub-directories <code class=\"language-text\">notebooks/01_collect_data</code> and <code class=\"language-text\">notebooks/02_analyses</code> depending on what stage in the argo workflow they belong to. This should make it explicit where notebooks go in the argo workflow dependency tree when defining it in the <code class=\"language-text\">wftmpl.yaml</code> manifest file. Ideally, the notebooks in <code class=\"language-text\">notebooks/01_collect_data</code> should not be dependent on each other (they could be run in parallel) and notebooks in <code class=\"language-text\">notebooks/02_analyses</code> should be independent of each other and only depend on the output of notebooks in <code class=\"language-text\">notebooks/01_collect_date</code>. That way we keep the workflow and dependency structure clear during development and we believe this architecture can be easily modified to accommodate more complex dependency requirements.</p>\n<ul class=\"pf-c-list\">\n<li>\n<p><strong>01<em>collect</em>data</strong></p>\n<ul class=\"pf-c-list\">\n<li><a href=\"../notebooks/01_collect_data/collect_data.ipynb\">collect_data</a> - Download new data from source and push to remote storage</li>\n<li><a href=\"../notebooks/01_collect_data/download_datasets.ipynb\">download_dataset</a> - Download existing preprocessed data from remote storage</li>\n<li><a href=\"../notebooks/01_collect_data/gz_to_raw.ipynb\">gz<em>to</em>raw</a> - Convert downloaded *.gz files to raw mbox format)</li>\n<li><a href=\"../notebooks/01_collect_data/raw_to_meta.ipynb\">raw<em>to</em>meta</a> - Process mbox files into monthly metadata *.csv and push to remote storage</li>\n<li><a href=\"../notebooks/01_collect_data/raw_to_text.ipynb\">raw<em>to</em>text</a> - Process mbox files into monthly email body *.csv and push to remote storage</li>\n<li>? (please <a href=\"https://github.com/aicoe-aiops/mailing-list-analysis-toolkit/issues/new?assignees=&#x26;labels=enhancement&#x26;template=feature_request.md\">open an issue</a> if you would like an additional data collection or pre processing step added)</li>\n</ul>\n</li>\n<li>\n<p><strong>02_analyses</strong></p>\n<ul class=\"pf-c-list\">\n<li><a href=\"../notebooks/02_analyses/contributor_analysis.ipynb\">contributor_analysis</a> (Quantify users monthly activity and push to remote storage)</li>\n<li><a href=\"../notebooks/02_analyses/contributor_analysis.ipynb\">keyword_analysis</a> (Identify top Keywords for each month and push to remote storage)</li>\n<li><a href=\"../notebooks/02_analyses/sentiment_analysis.ipynb\">sentiment analysis</a> (Sentiment Analysis on body of emails)</li>\n<li>? (please <a href=\"https://github.com/aicoe-aiops/mailing-list-analysis-toolkit/issues/new?assignees=&#x26;labels=enhancement&#x26;template=feature_request.md\">open an issue</a> if you would like an additional analysis added)</li>\n</ul>\n</li>\n</ul>","fields":{"srcLink":"https://github.com/aicoe-aiops/mailing-list-analysis-toolkit/blob/master/docs/content.md"},"frontmatter":{"title":"","description":null,"extraClasses":null}}},"pageContext":{"id":"6d9e36c9-011e-5b54-be3b-f3d28ea36cd1"}},"staticQueryHashes":["117426894","3000541721","3606484676","533861647"]}